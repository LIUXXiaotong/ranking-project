---
title: "Study1_data_analysis"
author: "Tong"
date: "11/10/2021"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(afex)
library(emmeans)
```


```{r read in data}
df <- read.csv("study_1_df.csv")
df_demo <- read.csv("df_data_demo2.csv")
df_demo_before <- read.csv("df_data_demo.csv")
```

### Descriptive statistics about participants: what are participants like and what do they say?


See below, the comments that participants gave:
```{r }

unique(df_demo$comments)
```

See below, the comments that all participants gave without any exclusions:
- I will not exclude this in the manuscript, just for our information.
```{r }

unique(df_demo_before$comments)
```


See below, MEAN and SD age of the participants
```{r }

mean(df_demo$age %>% as.numeric())
sd(df_demo$age %>% as.numeric())

#ggplot(data_demo, aes(x=age)) + geom_bar()
```

See below the gender distribution of participants. 
```{r }
df_demo %>% group_by(gender) %>% count()
```




### Descriptive statistics, multinomial probability distribution.

-  The average rate of providing a ranking with logical errors is around 0.39 (or, 0.3893597 to be more precise).
-  The average rate of providing a ranking with logical errors under the "ranking middle events only" condition is around 0.51 (or 0.5131827	 to be more precise).	
-  The average rate of providing a ranking with logical errors under the "ranking edge events only" condition is around 0.27	 (or 0.2655367 to be more precise).

/n

- For the "ties allowed" condition:

- The probability of giving ties is around 0.19 (0.1904762).
/n
- Under the condition "ranking middle events only", the conditional probability of proving type 1 logically incorrect ranking is 0.41554054, the conditional probability of providing type 2 logically incorrect ranking is 0.5033784, and the conditional probability of providing type 3 logically incorrect ranking is 0.08108108.
- Under the condition "ranking edge events only", the conditional probability of proving type 1 logically incorrect ranking is 0.03012048, the conditional probability of providing type 2 logically incorrect ranking is 0.94578313, and the conditional probability of providing type 3 logically incorrect ranking is 0.02409639.


What if conditional on the rankings being wrong and not belong to the type rankings?
When ranking indifferent events, the conditional probability of proving type 1 is 0.4522059. While when ranking extreme events only, the conditional probability of providing type 1 is 0.0308642.

/n

- For the "ties not allowed" condition:
- Under the condition "ranking middle events only", the conditional probability of proving type 1 logically incorrect ranking is 0.3815261, the conditional probability of providing type 2 logically incorrect ranking is 0.6184739.

- Under the condition "ranking edge events only", the conditional probability of proving type 1 logically incorrect ranking is  0.07758621, the conditional probability of providing type 2 logically incorrect ranking is 0.9224138.



```{r}
head(df)
str(df)

mean(df$if_there_are_errors) 

df %>%
  filter(within_subject_condition == "indiff") %>%
  summarise(mean_error = mean(if_there_are_errors))

df %>%
  filter(within_subject_condition == "extreme") %>%
  summarise(mean_error = mean(if_there_are_errors))


df_ties_allowed <- df %>% filter(between_subject_condition == "ties_allowed") 

## function to calculate conditional prob conditional on already being wrong
con_prob_error_type <- function(df){
  
  no_of_rankings_with_a_logical_error <- df %>% 
  select(ID, error_type) %>%
  drop_na() %>% 
  nrow() 
  
  
  no_of_type1 <- df %>%
  select(ID, error_type) %>%
  drop_na() %>%
  filter(error_type == "1") %>% 
  nrow()
  
  no_of_type2 <- df %>% 
  select(ID, error_type) %>%
  drop_na() %>%
  filter(error_type == "0") %>% 
  nrow()

  no_of_type3 <- df %>% 
  select(ID, error_type) %>%
  drop_na() %>%
  filter(error_type == "2") %>% 
  nrow()

  con_prob_type1 <- no_of_type1/no_of_rankings_with_a_logical_error
  con_prob_type2 <- no_of_type2/no_of_rankings_with_a_logical_error
  con_prob_type3 <- no_of_type3/no_of_rankings_with_a_logical_error
  
  return(c(con_prob_type1,  con_prob_type2,  con_prob_type3))
}
  


## apply the above two functions 
con_prob_error_type(df_ties_allowed %>%
  filter(within_subject_condition == "indiff") )

con_prob_error_type( df_ties_allowed %>%
  filter(within_subject_condition == "extreme") )

df_ties_not_allowed <- df %>% filter(between_subject_condition == "ties_not_allowed")


con_prob_error_type(df_ties_not_allowed %>%
  filter(within_subject_condition == "indiff") )

con_prob_error_type(df_ties_not_allowed %>%
  filter(within_subject_condition == "extreme") )


##  calculate the prob. of providing ties.
mean(df_ties_allowed$if_there_are_ties)


##  another way to calculate con prob for the condition where ties are allowed
 
 df_ties_allowed %>% 
  filter( within_subject_condition == "indiff" ) %>%
   select(ID, error_type) %>%
  drop_na() %>% 
  filter(error_type != 2) %>%
  summarise(con_type1 = mean(error_type),
            con_type2 = 1-con_type1)


  df_ties_allowed %>% 
  filter( within_subject_condition == "extreme" ) %>%
   select(ID, error_type) %>%
  drop_na() %>% 
  filter(error_type != 2) %>%
  summarise(con_type1 = mean(error_type),
            con_type2 = 1-con_type1)
 


```



```{r}
with(df, table(if_there_are_errors, error_type, useNA = "ifany"))

with(df, table(between_subject_condition, classify_all_ranks, useNA = "ifany"))

mtab <- df %>% 
  group_by(within_subject_condition, between_subject_condition) %>% 
  count(classify_all_ranks) %>% 
  mutate(prop = n / sum(n))

mtab %>% 
  pivot_wider(
    id_cols = c(between_subject_condition, within_subject_condition), 
    names_from = classify_all_ranks, 
    values_from = prop)


df %>% 
  filter(classify_all_ranks != "logical") %>% 
  group_by(within_subject_condition, between_subject_condition) %>% 
  count(classify_all_ranks) %>% 
  mutate(prop = n / sum(n)) %>% 
  pivot_wider(
    id_cols = c(between_subject_condition, within_subject_condition), 
    names_from = classify_all_ranks, 
    values_from = prop)

```




### Analysis DV1: if there are logical errors or not.

```{r echo=TRUE, warning=FALSE}


a1 <- aov_ez("ID", "if_there_are_errors", df,  between = "between_subject_condition", within  = "within_subject_condition")
a1
emmeans(a1, c("within_subject_condition", "between_subject_condition"))


# afex plot
afex_plot(a1, "within_subject_condition", "between_subject_condition") + 
  ylab(expression(paste("Probabilities of providing a ranking with logical errors"))) + 
  xlab("within-subjects conditions") + 
  theme(plot.margin = margin(l = 20)) +
  scale_x_discrete(labels=c("indiff" = "ranking indifferent events", "extreme" = "ranking extreme events"))
# people are more error-prone under condition A, where ties are allowed

ggsave("p1.jpg")
```

### Analysis DV2: conditional probabilities of making type 1 errors giving that there are errors in the rankings.
```{r}


## not sure if we can integrate two between-subject conditions.
 DV2_df <- df %>%
   select(ID, between_subject_condition, within_subject_condition, error_type) %>%
  drop_na() %>% 
  filter(error_type != 2)


a2 <- aov_ez("ID", "error_type", DV2_df, between = "between_subject_condition", within  = "within_subject_condition")
a2

emmeans(a2, c("within_subject_condition", "between_subject_condition"))


# afex plot
afex_plot(a2, "within_subject_condition", "between_subject_condition") + 
  ylab(expression(paste("Conditional probabilities of giving \n type 1 logically incorrect rankings"))) + 
  xlab("within-subjects conditions") + 
  theme(plot.margin = margin(l = 40)) +
  scale_x_discrete(labels=c("indiff" = "ranking indifferent events", "extreme" = "ranking extreme events"))

# first we do it separately for two datasets, namely df_ties_allowed and namely df_ties_not_allowed

# Let's start with df_ties_allowed

ggsave("conditional probability.jpg")

```


### Analysis DV3: the probabilities of giving type 1 errors



### Analysis DV4: the probabilities of giving ties. Can only analysis this DV with participants in the "ties_allowed" condition.


- For the "ties allowed" condition:

- The probability of giving ties is 0.1904762.


```{r}

```




To do for me:

- complete the simulation for "non-tied" events with sample size ranging from 1-20, 25, 30, 35. --- 50. This task is now running 
- complete the simulation to investigate what is the probabilities of providing ties if we range the sample size from 1-20, 25, 30, 35. --- 50.
- informative hypothesis testing (Herbert Hoijtink), allows us to test for example, |type2 – type1| > |type1 – type3|

